// src/components/VoiceRecorder.js
// Day 11: Whisper APIçµ±åˆç‰ˆ

import React, { useState, useRef, useEffect } from 'react';
import {
  View,
  Text,
  StyleSheet,
  TouchableOpacity,
  Animated,
  Alert,
  ActivityIndicator,
} from 'react-native';
import { Audio } from 'expo-av';
import Toast from 'react-native-toast-message';
import {
  transcribeAudioWithWhisper,
  isSpeechRecognitionAvailable,
  getSpeechErrorMessage,
} from '../services/speechService';

export default function VoiceRecorder({ onRecordingComplete, disabled = false }) {
  const [recording, setRecording] = useState(null);
  const [isRecording, setIsRecording] = useState(false);
  const [recordingDuration, setRecordingDuration] = useState(0);
  const [permissionResponse, requestPermission] = Audio.usePermissions();
  const [isTranscribing, setIsTranscribing] = useState(false);
  const [speechAvailable, setSpeechAvailable] = useState(true);

  const pulseAnim = useRef(new Animated.Value(1)).current;
  const pulseAnimLoopRef = useRef(null);
  const durationInterval = useRef(null);
  const isStartingRef = useRef(false);
  const isMountedRef = useRef(true);
  const recordingRef = useRef(null);
  const isTranscribingRef = useRef(false);
  const didCompleteRef = useRef(false);

  useEffect(() => {
    return () => {
      isMountedRef.current = false;
    };
  }, []);

  useEffect(() => {
    recordingRef.current = recording;
  }, [recording]);

  useEffect(() => {
    isTranscribingRef.current = isTranscribing;
  }, [isTranscribing]);

  useEffect(() => {
    return () => {
      (async () => {
        try {
          if (recordingRef.current) {
            await recordingRef.current.stopAndUnloadAsync();
          }
        } catch (err) {
          // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ™‚ã®ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
        }

        try {
          await Audio.setAudioModeAsync({ allowsRecordingIOS: false });
        } catch (err) {
          // ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ™‚ã®ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–
        }
      })();
    };
  }, []);

  useEffect(() => {
    checkSpeechAvailability();
  }, []);

  const checkSpeechAvailability = async () => {
    const available = await isSpeechRecognitionAvailable();
    setSpeechAvailable(available);
  };

  // éŒ²éŸ³é–¢é€£ã®ã‚¹ãƒ†ãƒ¼ãƒˆã‚’åˆæœŸåŒ–ã™ã‚‹ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
  const resetRecordingState = () => {
    setIsRecording(false);
    setRecordingDuration(0);
    setIsTranscribing(false);
  };

  useEffect(() => {
    if (isRecording) {
      const loop = Animated.loop(
        Animated.sequence([
          Animated.timing(pulseAnim, {
            toValue: 1.2,
            duration: 500,
            useNativeDriver: true,
          }),
          Animated.timing(pulseAnim, {
            toValue: 1,
            duration: 500,
            useNativeDriver: true,
          }),
        ])
      );
      pulseAnimLoopRef.current = loop;
      loop.start();

      durationInterval.current = setInterval(() => {
        setRecordingDuration((prev) => prev + 1);
      }, 1000);
    } else {
      pulseAnim.setValue(1);
      if (durationInterval.current) {
        clearInterval(durationInterval.current);
      }
      if (pulseAnimLoopRef.current) {
        pulseAnimLoopRef.current.stop();
      }
    }

    return () => {
      if (durationInterval.current) {
        clearInterval(durationInterval.current);
      }
      if (pulseAnimLoopRef.current) {
        pulseAnimLoopRef.current.stop();
      }
    };
  }, [isRecording]);

  const startRecording = async () => {
    if (isStartingRef.current || isRecording) {
      return;
    }
    isStartingRef.current = true;

    try {
      if (!permissionResponse || permissionResponse.status !== 'granted') {
        const permission = await requestPermission();
        if (!permission.granted) {
          Alert.alert(
            'æ¨©é™ãŒå¿…è¦ã§ã™',
            'ãƒã‚¤ã‚¯ã¸ã®ã‚¢ã‚¯ã‚»ã‚¹æ¨©é™ãŒå¿…è¦ã§ã™ã€‚è¨­å®šã‹ã‚‰æ¨©é™ã‚’æœ‰åŠ¹ã«ã—ã¦ãã ã•ã„ã€‚'
          );
          return;
        }
      }

      await Audio.setAudioModeAsync({
        allowsRecordingIOS: true,
        playsInSilentModeIOS: true,
        staysActiveInBackground: true,
      });

      const { recording } = await Audio.Recording.createAsync(
        Audio.RecordingOptionsPresets.HIGH_QUALITY
      );

      setRecording(recording);
      setIsRecording(true);
      setRecordingDuration(0);

      console.log('[VoiceRecorder] éŒ²éŸ³é–‹å§‹');

    } catch (err) {
      console.error('[VoiceRecorder] éŒ²éŸ³é–‹å§‹ã‚¨ãƒ©ãƒ¼:', err);
      Alert.alert('ã‚¨ãƒ©ãƒ¼', 'éŒ²éŸ³ã‚’é–‹å§‹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ã‚‚ã†ä¸€åº¦ãŠè©¦ã—ãã ã•ã„ã€‚');
    } finally {
      isStartingRef.current = false;
    }
  };

  const stopRecording = async () => {
    if (!recording) return;

    if (didCompleteRef.current) {
      return;
    }

    try {
      setIsRecording(false);

      // éŒ²éŸ³ã‚’åœæ­¢
      await recording.stopAndUnloadAsync();
      
      await Audio.setAudioModeAsync({
        allowsRecordingIOS: false,
      });

      const uri = recording.getURI();
      const duration = recordingDuration;

      setRecording(null);
      setRecordingDuration(0);

      console.log('[VoiceRecorder] éŒ²éŸ³å®Œäº†:', { uri, duration });

      // æ–‡å­—èµ·ã“ã—é–‹å§‹
      if (uri && speechAvailable) {
        setIsTranscribing(true);

        try {
          console.log('[VoiceRecorder] æ–‡å­—èµ·ã“ã—é–‹å§‹...');
          
          const text = await transcribeAudioWithWhisper(uri);

          console.log('[VoiceRecorder] æ–‡å­—èµ·ã“ã—å®Œäº†:', text);

          // ãƒ†ã‚­ã‚¹ãƒˆã‚’è¦ªã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆã«æ¸¡ã™
          if (onRecordingComplete) {
            didCompleteRef.current = true;
            onRecordingComplete(text, duration);
          }

        } catch (transcribeError) {
          console.error('[VoiceRecorder] æ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼:', transcribeError);

          // ã‚¨ãƒ©ãƒ¼æ™‚ã®ã‚¢ãƒ©ãƒ¼ãƒˆ
          Alert.alert(
            'æ–‡å­—èµ·ã“ã—ã‚¨ãƒ©ãƒ¼',
            getSpeechErrorMessage(transcribeError),
            [
              {
                text: 'ã‚­ãƒ¼ãƒœãƒ¼ãƒ‰ã§å…¥åŠ›',
                onPress: () => {
                  if (onRecordingComplete) {
                    didCompleteRef.current = true;
                    onRecordingComplete('', duration);
                  }
                }
              }
            ]
          );
        } finally {
          setIsTranscribing(false);
        }
      } else {
        // éŸ³å£°èªè­˜ãŒåˆ©ç”¨ã§ããªã„å ´åˆ
        Toast.show({
          type: 'info',
          text1: 'éŒ²éŸ³å®Œäº† ğŸ¤',
          text2: 'æ–‡å­—èµ·ã“ã—æ©Ÿèƒ½ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚æ‰‹å‹•ã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚',
          position: 'top',
          visibilityTime: 4000,
        });

        if (onRecordingComplete) {
          didCompleteRef.current = true;
          onRecordingComplete('', duration);
        }
      }

    } catch (err) {
      console.error('[VoiceRecorder] éŒ²éŸ³åœæ­¢ã‚¨ãƒ©ãƒ¼:', err);
      Alert.alert('ã‚¨ãƒ©ãƒ¼', 'éŒ²éŸ³ã®ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸã€‚');
      resetRecordingState();
    } finally {
      setTimeout(() => {
        if (isMountedRef.current) {
          didCompleteRef.current = false;
        }
      }, 300);
      if (isMountedRef.current) {
        setIsTranscribing(false);
      }
    }
  };

  const cancelRecording = async () => {
    if (!recording) return;

    try {
      setIsRecording(false);

      await recording.stopAndUnloadAsync();
      
      await Audio.setAudioModeAsync({
        allowsRecordingIOS: false,
      });

      setRecording(null);
      setRecordingDuration(0);

      console.log('[VoiceRecorder] éŒ²éŸ³ã‚­ãƒ£ãƒ³ã‚»ãƒ«');

    } catch (err) {
      console.error('[VoiceRecorder] éŒ²éŸ³ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã‚¨ãƒ©ãƒ¼:', err);
      resetRecordingState();
    }
  };

  const formatDuration = (seconds) => {
    const mins = Math.floor(seconds / 60);
    const secs = seconds % 60;
    return `${mins.toString().padStart(2, '0')}:${secs.toString().padStart(2, '0')}`;
  };

  return (
    <View style={styles.container}>
      {/* æ–‡å­—èµ·ã“ã—ä¸­ã®è¡¨ç¤º */}
      {isTranscribing && (
        <View style={styles.transcribingContainer}>
          <ActivityIndicator size="large" color="#2196F3" />
          <Text style={styles.transcribingTitle}>æ–‡å­—èµ·ã“ã—ä¸­...</Text>
          <Text style={styles.transcribingText}>
            AIãŒã‚ãªãŸã®éŸ³å£°ã‚’ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã—ã¦ã„ã¾ã™
          </Text>
          <Text style={styles.transcribingHint}>
            30ç§’ã»ã©ãŠå¾…ã¡ãã ã•ã„
          </Text>
        </View>
      )}

      {/* éŒ²éŸ³ãƒœã‚¿ãƒ³/éŒ²éŸ³ä¸­UIï¼ˆæ–‡å­—èµ·ã“ã—ä¸­ã¯éè¡¨ç¤ºï¼‰ */}
      {!isTranscribing && (
        <>
          {!isRecording ? (
            <TouchableOpacity
              style={[styles.recordButton, disabled && styles.recordButtonDisabled]}
              onPress={startRecording}
              disabled={disabled}
            >
              <Text style={styles.recordButtonIcon}>ğŸ¤</Text>
              <Text style={styles.recordButtonText}>éŸ³å£°ã§å›ç­”ã™ã‚‹</Text>
              <Text style={styles.recordButtonHint}>
                {speechAvailable 
                  ? 'ã‚¿ãƒƒãƒ—ã—ã¦éŒ²éŸ³ï¼‹è‡ªå‹•æ–‡å­—èµ·ã“ã—' 
                  : 'ã‚¿ãƒƒãƒ—ã—ã¦éŒ²éŸ³é–‹å§‹ï¼ˆæ–‡å­—èµ·ã“ã—ä¸å¯ï¼‰'}
              </Text>
            </TouchableOpacity>
          ) : (
            <View style={styles.recordingContainer}>
              <Animated.View
                style={[
                  styles.recordingIndicator,
                  { transform: [{ scale: pulseAnim }] },
                ]}
              >
                <View style={styles.recordingDot} />
              </Animated.View>

              <View style={styles.recordingInfo}>
                <Text style={styles.recordingText}>éŒ²éŸ³ä¸­...</Text>
                <Text style={styles.recordingDuration}>
                  {formatDuration(recordingDuration)}
                </Text>
              </View>

              <View style={styles.recordingActions}>
                <TouchableOpacity
                  style={[styles.actionButton, styles.cancelButton]}
                  onPress={cancelRecording}
                >
                  <Text style={styles.actionButtonText}>âœ•</Text>
                </TouchableOpacity>

                <TouchableOpacity
                  style={[styles.actionButton, styles.stopButton]}
                  onPress={stopRecording}
                >
                  <Text style={styles.actionButtonText}>â¬›</Text>
                </TouchableOpacity>
              </View>
            </View>
          )}

          <View style={styles.notice}>
            <Text style={styles.noticeIcon}>
              {speechAvailable ? 'âœ…' : 'â„¹ï¸'}
            </Text>
            <Text style={styles.noticeText}>
              {speechAvailable 
                ? 'éŒ²éŸ³åœæ­¢å¾Œã€OpenAI Whisper APIã§è‡ªå‹•çš„ã«éŸ³å£°ã‚’æ–‡å­—èµ·ã“ã—ã—ã¾ã™ã€‚å®Œäº†å¾Œã€ãƒ†ã‚­ã‚¹ãƒˆã‚’ç·¨é›†ã§ãã¾ã™ã€‚'
                : 'éŸ³å£°èªè­˜ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚éŒ²éŸ³å¾Œã€ãƒ†ã‚­ã‚¹ãƒˆã§å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚'}
            </Text>
          </View>
        </>
      )}
    </View>
  );
}

const styles = StyleSheet.create({
  container: {
    marginBottom: 20,
  },
  recordButton: {
    backgroundColor: '#fff',
    borderRadius: 12,
    padding: 20,
    alignItems: 'center',
    justifyContent: 'center',
    borderWidth: 2,
    borderColor: '#2196F3',
  },
  recordButtonDisabled: {
    borderColor: '#e0e0e0',
    opacity: 0.5,
  },
  recordButtonIcon: {
    fontSize: 40,
    marginBottom: 8,
  },
  recordButtonText: {
    fontSize: 16,
    fontWeight: 'bold',
    color: '#2196F3',
    marginBottom: 4,
  },
  recordButtonHint: {
    fontSize: 12,
    color: '#999',
    textAlign: 'center',
  },
  recordingContainer: {
    backgroundColor: '#fff',
    borderRadius: 12,
    padding: 20,
    borderWidth: 2,
    borderColor: '#FF5252',
  },
  recordingIndicator: {
    alignItems: 'center',
    marginBottom: 16,
  },
  recordingDot: {
    width: 20,
    height: 20,
    borderRadius: 10,
    backgroundColor: '#FF5252',
  },
  recordingInfo: {
    alignItems: 'center',
    marginBottom: 16,
  },
  recordingText: {
    fontSize: 18,
    fontWeight: 'bold',
    color: '#FF5252',
    marginBottom: 8,
  },
  recordingDuration: {
    fontSize: 24,
    fontWeight: 'bold',
    color: '#333',
    fontVariant: ['tabular-nums'],
  },
  recordingActions: {
    flexDirection: 'row',
    justifyContent: 'center',
    gap: 16,
  },
  actionButton: {
    width: 60,
    height: 60,
    borderRadius: 30,
    alignItems: 'center',
    justifyContent: 'center',
  },
  cancelButton: {
    backgroundColor: '#757575',
  },
  stopButton: {
    backgroundColor: '#FF5252',
  },
  actionButtonText: {
    fontSize: 24,
    color: '#fff',
  },
  transcribingContainer: {
    alignItems: 'center',
    padding: 32,
    backgroundColor: '#E3F2FD',
    borderRadius: 12,
    marginBottom: 16,
  },
  transcribingTitle: {
    fontSize: 18,
    fontWeight: 'bold',
    color: '#2196F3',
    marginTop: 16,
    marginBottom: 8,
  },
  transcribingText: {
    fontSize: 14,
    color: '#666',
    textAlign: 'center',
    marginBottom: 8,
  },
  transcribingHint: {
    fontSize: 12,
    color: '#999',
    textAlign: 'center',
  },
  notice: {
    flexDirection: 'row',
    backgroundColor: '#E3F2FD',
    borderRadius: 8,
    padding: 12,
    marginTop: 12,
  },
  noticeIcon: {
    fontSize: 16,
    marginRight: 8,
  },
  noticeText: {
    flex: 1,
    fontSize: 12,
    color: '#1976D2',
    lineHeight: 18,
  },
});